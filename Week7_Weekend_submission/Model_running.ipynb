{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f08fd45e-25fe-4890-9fec-7950ea8add29",
   "metadata": {},
   "source": [
    "## Model predictions using the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31aeac5c-a8d5-483b-a467-d21eaf3703be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eac1bad1-8a84-4700-8a64-072d3bdd4f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"C:/Users/Srinidhi/Documents/USF/Data_Science_Programming/Week7_Weekend_submission/X_train.csv\")\n",
    "X_test = pd.read_csv(\"C:/Users/Srinidhi/Documents/USF/Data_Science_Programming/Week7_Weekend_submission/X_test.csv\")\n",
    "y_train = pd.read_csv(\"C:/Users/Srinidhi/Documents/USF/Data_Science_Programming/Week7_Weekend_submission/y_train.csv\")\n",
    "y_test = pd.read_csv(\"C:/Users/Srinidhi/Documents/USF/Data_Science_Programming/Week7_Weekend_submission/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e25c7019-acb3-4e5f-aeef-ce62572d3aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(548, 1472)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44eea0a1-9d74-4e6f-a2ad-e12cdf955fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58304, 1472)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62085177-8a4e-484d-ac0a-ccdb7941a649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(548, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46b40944-64d2-48a6-b490-c4b72be6b954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58304, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4760cb-aed3-4744-a284-41d6641376cb",
   "metadata": {},
   "source": [
    "Moving forward I will be choosing F1 score to evaluate the performance of the model. I am choosing this scoring metric because it combines both precision and recall value into one scoring metric by defining the harmonic mean and works best for this kind of probelms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09041c0-25dc-44cb-9f0f-640b98f02c4f",
   "metadata": {},
   "source": [
    "# Initial random search based on the parameters set for the Random search for the logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8c4643d-bfc5-40cf-b98b-e4dd27c62882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 3 is smaller than n_iter=100. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9779602625902779\n",
      "... with parameters: {'penalty': 'l2', 'C': 10}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 3\n",
    "\n",
    "param_grid = [\n",
    "    { 'penalty': ['l2'], 'C': [ 0.1, 1, 10]}\n",
    "]\n",
    "\n",
    "Lr = LogisticRegression()\n",
    "rand_search = RandomizedSearchCV(estimator = Lr, param_distributions=param_grid, cv=kfolds, n_iter=100,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7879060e-40a9-4f45-9d02-a0f117db2a9f",
   "metadata": {},
   "source": [
    "## Grid search for the logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f621e55-2c69-4092-8203-440dbee8e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "25 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.80977723        nan 0.88569615        nan 0.97012285\n",
      "        nan 0.97420683        nan 0.97240533]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [       nan 0.85419407        nan 0.9393384         nan 0.98757345\n",
      "        nan 0.99727065        nan 0.99817974]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9742068308729206\n",
      "... with parameters: {'C': 10, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = [\n",
    "    { 'penalty': ['l1', 'l2'], 'C': [0.01, 0.1, 1, 10, 100]}\n",
    "]\n",
    "\n",
    "Lr_grid = LogisticRegression()\n",
    "grid_search = GridSearchCV(estimator = Lr_grid, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f242c3-ee1f-440d-9ca6-9f2427b7c480",
   "metadata": {},
   "source": [
    "### Fit a SVM classification model using Random Search for Linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bff212b1-3d4f-4fba-87e6-63902aac6fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 7 is smaller than n_iter=500. Running 7 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9724053296218781\n",
      "... with parameters: {'kernel': 'linear', 'C': 1}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "     'C': [0.0001, 0.001, 0.1, 1, 10, 100, 1000], \n",
    "    'kernel': ['linear']\n",
    "}\n",
    "\n",
    "rand_linear_SVC = SVC()\n",
    "rand_search = RandomizedSearchCV(estimator = rand_linear_SVC, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestPrecisionTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c797acc4-04db-4234-aaf7-5b694f7ac2fb",
   "metadata": {},
   "source": [
    "### Fit a SVM classification model using Grid Search for Linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91b2ee3b-6ea9-40ca-9287-64a2a2be576a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 7 candidates, totalling 70 fits\n",
      "The best f1 score is 0.9776157863464366\n",
      "... with parameters: {'C': 10, 'kernel': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 10\n",
    "\n",
    "param_grid = {\n",
    "     'C': [0.0001, 0.001, 0.1, 1, 10, 100, 1000], \n",
    "    'kernel': ['linear']\n",
    "}\n",
    "\n",
    "Grid_Linear_SVC = SVC()\n",
    "grid_search = GridSearchCV(estimator = Grid_Linear_SVC, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestPrecisionTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e22d704-b443-4fe9-bb2a-c7de16e45d49",
   "metadata": {},
   "source": [
    "### Fit a SVM classification model using Random Search for Poly kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b93f4c25-6d20-417b-8f8f-28d0cbd60b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 16 is smaller than n_iter=500. Running 16 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 16 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9736799574694313\n",
      "... with parameters: {'kernel': 'poly', 'gamma': 1, 'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 10\n",
    "\n",
    "param_grid = {\n",
    "     'C': [0.1,1, 10, 100], \n",
    "    'gamma': [1,0.1,0.01,0.001],\n",
    "    'kernel': ['poly']\n",
    "}\n",
    "\n",
    "Random_Poly_SVC = SVC()\n",
    "rand_search = RandomizedSearchCV(estimator = Random_Poly_SVC, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestPrecisionTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3610dac6-730f-4918-a85c-e7b3a5cb39ee",
   "metadata": {},
   "source": [
    "### Fit a SVM classification model using Grid Search for Poly kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "968bffb8-9bab-470a-8472-32df9b83b8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9724053296218781\n",
      "... with parameters: {'C': 0.1, 'gamma': 1, 'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "     'C': [0.1,1, 10, 100], \n",
    "    'gamma': [1,0.1,0.01,0.001],\n",
    "    'kernel': ['poly']\n",
    "}\n",
    "\n",
    "\n",
    "Grid_Poly_SVC = SVC()\n",
    "grid_search = GridSearchCV(estimator = Grid_Poly_SVC, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestPrecisionTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7047c2-5ee2-4bb2-81f4-53927b3c6318",
   "metadata": {},
   "source": [
    "### Fit a SVM classification model using Random Search for RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "947a14d1-01c1-4a6a-a9cc-5fc7a0a01b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 9 is smaller than n_iter=500. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9757976045282547\n",
      "... with parameters: {'kernel': 'rbf', 'gamma': 0.1, 'C': 10}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 10\n",
    "\n",
    "param_grid = {\n",
    "     'C': [0.1,1, 10], \n",
    "    'gamma': [1,0.1,0.011],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "Random_rbf_SVC = SVC()\n",
    "rand_search = RandomizedSearchCV(estimator = Random_rbf_SVC, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestPrecisionTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc28099-a15f-41ae-a3dd-bc0b9340f58a",
   "metadata": {},
   "source": [
    "### Fit a SVM classification model using Grid Search for rbf kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98a1069c-2dae-4893-be38-5a48c413c99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "25 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.9601933  0.9601933         nan 0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933         nan 0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.98036633\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      "        nan 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933         nan\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.96222476 0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.98036633 0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9621563  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.97299714\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.98728732 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9621563  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933         nan 0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.95842426 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.98371445 0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.96418775 0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.96559657 0.95603428\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.95603428 0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.97496034 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.96048848 0.96450111 0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.95845702 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.95842426 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.97496034 0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.97299714 0.96559657 0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.96048848\n",
      " 0.9601933  0.9601933 ]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.96356898 0.96356898        nan 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898        nan 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.98695939\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      "        nan 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898        nan\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96929721 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96754717 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.98695939 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96597514 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.97917725\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.99450583 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96597514 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898        nan 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96559346 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.98670586 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.97079877 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96933257 0.96434208\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96434208 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.980793   0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.98558138 0.97235403 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.97393219 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96559346 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.980793   0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.98318501 0.96846021 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.98558138\n",
      " 0.96356898 0.96356898]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9872873200854851\n",
      "... with parameters: {'min_samples_split': 6, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.004600000000000001, 'max_leaf_nodes': 78, 'max_depth': 14, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,100),  \n",
    "    'min_samples_leaf': np.arange(1,100),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 100), \n",
    "    'max_depth': np.arange(1,50), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08463c0a-da11-441b-bad5-4363f1cdbd50",
   "metadata": {},
   "source": [
    "## Initial random search based on the parameters set for the Random search for the Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74b2f9ad-dc89-4133-9870-7661569cc6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "20 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.9601933         nan 0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.95842426 0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.98554901 0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.96222476\n",
      " 0.9601933  0.98554901 0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.96559657 0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.95845702 0.9601933  0.9601933\n",
      " 0.97427671 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.96196021 0.9601933\n",
      " 0.9601933  0.96914417 0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.96720388\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.96422959 0.9601933  0.9601933  0.95845702\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.96559657 0.9601933\n",
      " 0.9601933  0.9601933  0.98210464 0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.96623856 0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.95603428 0.9601933  0.97260899\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933         nan 0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.98036633 0.9601933  0.9601933  0.9601933\n",
      " 0.96819785 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.96050666 0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.95845702 0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.97669389 0.96559657 0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.98912218 0.96450111 0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.97299714\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.96773322 0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      "        nan 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.96200737\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.97672997 0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.96929585 0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.97669389 0.9601933  0.9601933\n",
      " 0.97988232 0.9601933  0.9601933  0.9601933  0.9601933  0.95842426\n",
      " 0.9709138         nan 0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.96200737\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.95845702 0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.96450111 0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.96929585 0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.96914417 0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.95845702 0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.95842426\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933  0.9601933  0.9601933  0.9601933  0.9601933\n",
      " 0.9601933  0.9601933 ]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.96356898        nan 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96559346 0.96356898 0.96356898 0.96732387\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.98822276 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96647842 0.96356898 0.96356898 0.96356898 0.96754717\n",
      " 0.96356898 0.9873664  0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96846021 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.97393219 0.96356898 0.96356898\n",
      " 0.97901798 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96624565 0.96356898\n",
      " 0.96356898 0.9789498  0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96647842 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.97044712\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.97543535 0.96356898 0.96356898 0.9724582\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96846021 0.96356898\n",
      " 0.96356898 0.96356898 0.9904943  0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.97404474 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96647842 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96434208 0.96356898 0.99545038\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898        nan 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.98695939 0.96356898 0.96356898 0.96356898\n",
      " 0.97289131 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96887912 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96604864 0.96356898 0.96356898 0.96356898 0.9748009  0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.98148468 0.96846021 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96732387 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.99183869 0.97279147 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96604864 0.96356898 0.97917725\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.97894553 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96647842 0.96356898 0.96356898 0.96356898 0.96356898\n",
      "        nan 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96647842 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.97436341\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96647842 0.98301146 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.97591625 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.98235761 0.96356898 0.96356898\n",
      " 0.98451387 0.96356898 0.96356898 0.96356898 0.96356898 0.96559346\n",
      " 0.98116826        nan 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.97436341\n",
      " 0.96356898 0.96647842 0.96356898 0.96356898 0.9724582  0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.97235403 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.97902531 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.9789498  0.96356898\n",
      " 0.96356898 0.96356898 0.96780612 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.97393219 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96559346\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898 0.96356898\n",
      " 0.96356898 0.96647842]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9891221824708063\n",
      "... with parameters: {'min_samples_split': 13, 'min_samples_leaf': 5, 'min_impurity_decrease': 0.0091, 'max_leaf_nodes': 11, 'max_depth': 33, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,100),  \n",
    "    'min_samples_leaf': np.arange(1,100),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 100), \n",
    "    'max_depth': np.arange(1,50), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d60cda4-30f0-4578-9a23-e0c9a5c7e13c",
   "metadata": {},
   "source": [
    "## Exhaustive grid search based on the parameters set for the Grid search for the Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b05ddd05-ec20-4fd1-829b-e5f2429a5f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3000 candidates, totalling 15000 fits\n",
      "The best f1 score is 0.9891221824708063\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 30, 'max_leaf_nodes': 8, 'min_impurity_decrease': 0.0089, 'min_samples_leaf': 3, 'min_samples_split': 10}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(10,15),  \n",
    "    'min_samples_leaf': np.arange(3,7),\n",
    "    'min_impurity_decrease': np.arange(0.0089, 0.0095, 0.0001),\n",
    "    'max_leaf_nodes': np.arange(8,13), \n",
    "    'max_depth': np.arange(30,35), \n",
    "    'criterion': ['entropy'],\n",
    "}\n",
    "\n",
    "dtree_grid = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtree_grid, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1b4721-5976-4b91-84f1-7d4b9d9d81fb",
   "metadata": {},
   "source": [
    "## Neural Network using MLP Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ba36dd-1522-4acd-a86f-a503b2112673",
   "metadata": {},
   "source": [
    "### With Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22450fda-a7ce-40b6-9952-d6de5322b9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9759743531909016\n",
      "... with parameters: {'solver': 'adam', 'max_iter': 5000, 'learning_rate_init': 0.1, 'learning_rate': 'constant', 'hidden_layer_sizes': (40, 20), 'alpha': 1, 'activation': 'relu'}\n",
      "CPU times: total: 5.73 s\n",
      "Wall time: 35min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [ (50,), (70,),(50,30), (40,20), (60,40, 20), (70,50,40)],\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0, .2, .5, .7, 1],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1, 0.2, 0.5],\n",
    "    'max_iter': [5000]\n",
    "}\n",
    "\n",
    "ann = MLPClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = ann, param_distributions=param_grid, cv=kfolds, n_iter=100,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7870e5a8-b18f-4414-81a4-6d0a208a566f",
   "metadata": {},
   "source": [
    "### With Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d88dc58-da12-485e-a2a3-60eabdf45b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score is 0.9777758544419441\n",
      "... with parameters: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (70,), 'learning_rate': 'adaptive', 'learning_rate_init': 0.01, 'max_iter': 5000, 'solver': 'adam'}\n",
      "CPU times: total: 20.2 s\n",
      "Wall time: 49min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "score_measure = \"f1\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [ (30,), (50,), (70,), (90,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [.5, .7, 1],\n",
    "    'learning_rate': ['constant', 'adaptive', 'invscaling'],\n",
    "    'learning_rate_init': [0.001, 0.005, 0.01, 0.15],\n",
    "    'max_iter': [5000]\n",
    "}\n",
    "\n",
    "ann = MLPClassifier()\n",
    "grid_search = GridSearchCV(estimator = ann, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04cb1017-d002-4e04-891f-138fb64ae831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05fe75f6-b8d4-417d-ba1e-bd8e8f05ac22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 234 ms\n",
      "Wall time: 6.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create model stucture\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Input(1472))\n",
    "model.add(keras.layers.Dense(50, activation='relu', kernel_initializer='glorot_normal'))\n",
    "model.add(keras.layers.Dense(50, activation='relu', kernel_initializer='glorot_normal'))\n",
    "model.add(keras.layers.Dense(50, activation='relu', kernel_initializer='glorot_normal'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax', kernel_initializer='glorot_normal')) # final layer, 10 categories\n",
    "\n",
    "\n",
    "# compile\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "72d6655a-9a0b-4825-8e52-708099adb400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6/6 [==============================] - 30s 4s/step - loss: 2.2968 - accuracy: 0.1423 - val_loss: 2.2730 - val_accuracy: 0.0773\n",
      "Epoch 2/20\n",
      "6/6 [==============================] - 5s 894ms/step - loss: 2.2246 - accuracy: 0.5255 - val_loss: 2.2014 - val_accuracy: 0.1078\n",
      "Epoch 3/20\n",
      "6/6 [==============================] - 3s 548ms/step - loss: 2.1230 - accuracy: 0.5620 - val_loss: 2.0861 - val_accuracy: 0.0957\n",
      "Epoch 4/20\n",
      "6/6 [==============================] - 2s 403ms/step - loss: 1.9480 - accuracy: 0.5584 - val_loss: 1.8809 - val_accuracy: 0.1035\n",
      "Epoch 5/20\n",
      "6/6 [==============================] - 2s 387ms/step - loss: 1.6617 - accuracy: 0.5620 - val_loss: 1.5507 - val_accuracy: 0.1367\n",
      "Epoch 6/20\n",
      "6/6 [==============================] - 3s 548ms/step - loss: 1.2616 - accuracy: 0.5985 - val_loss: 1.1239 - val_accuracy: 0.2508\n",
      "Epoch 7/20\n",
      "6/6 [==============================] - 2s 376ms/step - loss: 0.8721 - accuracy: 0.7263 - val_loss: 0.7781 - val_accuracy: 0.5456\n",
      "Epoch 8/20\n",
      "6/6 [==============================] - 2s 374ms/step - loss: 0.6663 - accuracy: 0.8942 - val_loss: 0.6568 - val_accuracy: 0.6433\n",
      "Epoch 9/20\n",
      "6/6 [==============================] - 2s 362ms/step - loss: 0.5809 - accuracy: 0.8485 - val_loss: 0.6324 - val_accuracy: 0.5877\n",
      "Epoch 10/20\n",
      "6/6 [==============================] - 2s 362ms/step - loss: 0.5127 - accuracy: 0.9434 - val_loss: 0.5443 - val_accuracy: 0.7634\n",
      "Epoch 11/20\n",
      "6/6 [==============================] - 2s 375ms/step - loss: 0.4352 - accuracy: 0.9471 - val_loss: 0.5045 - val_accuracy: 0.7713\n",
      "Epoch 12/20\n",
      "6/6 [==============================] - 2s 393ms/step - loss: 0.3524 - accuracy: 0.9781 - val_loss: 0.4719 - val_accuracy: 0.7860\n",
      "Epoch 13/20\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 0.2652 - accuracy: 0.9763 - val_loss: 0.3446 - val_accuracy: 0.9111\n",
      "Epoch 14/20\n",
      "6/6 [==============================] - 2s 447ms/step - loss: 0.1907 - accuracy: 0.9909 - val_loss: 0.3635 - val_accuracy: 0.8714\n",
      "Epoch 15/20\n",
      "6/6 [==============================] - 2s 339ms/step - loss: 0.1266 - accuracy: 0.9927 - val_loss: 0.2734 - val_accuracy: 0.9487\n",
      "Epoch 16/20\n",
      "6/6 [==============================] - 2s 354ms/step - loss: 0.0828 - accuracy: 0.9927 - val_loss: 0.2241 - val_accuracy: 0.9733\n",
      "Epoch 17/20\n",
      "6/6 [==============================] - 2s 392ms/step - loss: 0.0572 - accuracy: 0.9909 - val_loss: 0.2634 - val_accuracy: 0.9516\n",
      "Epoch 18/20\n",
      "6/6 [==============================] - 2s 411ms/step - loss: 0.0418 - accuracy: 0.9945 - val_loss: 0.2040 - val_accuracy: 0.9752\n",
      "Epoch 19/20\n",
      "6/6 [==============================] - 2s 384ms/step - loss: 0.0332 - accuracy: 0.9927 - val_loss: 0.1820 - val_accuracy: 0.9780\n",
      "Epoch 20/20\n",
      "6/6 [==============================] - 2s 363ms/step - loss: 0.0267 - accuracy: 0.9945 - val_loss: 0.2348 - val_accuracy: 0.9518\n",
      "CPU times: total: 3min 51s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    epochs=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f55a317a-6d3a-423c-89e8-4b2fdc240f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.23477661609649658, 0.9517529010772705]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "scores\n",
    "# In results, first is loss, second is precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2238556b-892c-4806-86d2-da2146079dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.23\n",
      "accuracy: 95.18%\n"
     ]
    }
   ],
   "source": [
    "print(\"%s: %.2f\" % (model.metrics_names[0], scores[0]))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d7ebecd2-1ca2-4229-bc93-91bc1328546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Input(shape=1472))\n",
    "model.add(keras.layers.Dense(100, activation='relu', kernel_initializer='glorot_normal'))\n",
    "model.add(keras.layers.Dense(100, activation='relu', kernel_initializer='glorot_normal'))\n",
    "model.add(keras.layers.Dense(100, activation='relu', kernel_initializer='glorot_normal'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax', kernel_initializer='glorot_normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2740709c-7aa3-43b3-8566-caefa2d6ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e49d6b9e-f8ea-4296-bc91-6e3ac6de1e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6/6 [==============================] - 39s 4s/step - loss: 1.5455 - precision: 0.2090 - val_loss: 0.8405 - val_precision: 0.0026\n",
      "Epoch 2/20\n",
      "6/6 [==============================] - 4s 783ms/step - loss: 0.5953 - precision: 0.5018 - val_loss: 0.2480 - val_precision: 0.0026\n",
      "Epoch 3/20\n",
      "6/6 [==============================] - 3s 552ms/step - loss: 0.2056 - precision: 0.4982 - val_loss: 0.2083 - val_precision: 0.0026\n",
      "Epoch 4/20\n",
      "6/6 [==============================] - 3s 551ms/step - loss: 0.0369 - precision: 0.4982 - val_loss: 0.1261 - val_precision: 0.0026\n",
      "Epoch 5/20\n",
      "6/6 [==============================] - 2s 418ms/step - loss: 0.0225 - precision: 0.4928 - val_loss: 0.2424 - val_precision: 0.0026\n",
      "Epoch 6/20\n",
      "6/6 [==============================] - 3s 547ms/step - loss: 0.0119 - precision: 0.4910 - val_loss: 0.1753 - val_precision: 0.0026\n",
      "Epoch 7/20\n",
      "6/6 [==============================] - 2s 389ms/step - loss: 0.0143 - precision: 0.5072 - val_loss: 0.4481 - val_precision: 0.0026\n",
      "Epoch 8/20\n",
      "6/6 [==============================] - 2s 392ms/step - loss: 0.0086 - precision: 0.5054 - val_loss: 0.1780 - val_precision: 0.0026\n",
      "Epoch 9/20\n",
      "6/6 [==============================] - 2s 386ms/step - loss: 0.0121 - precision: 0.5072 - val_loss: 0.8067 - val_precision: 0.0026\n",
      "Epoch 10/20\n",
      "6/6 [==============================] - 2s 430ms/step - loss: 0.0122 - precision: 0.5000 - val_loss: 0.8973 - val_precision: 0.0026\n",
      "Epoch 11/20\n",
      "6/6 [==============================] - 2s 494ms/step - loss: 0.0064 - precision: 0.4946 - val_loss: 0.2328 - val_precision: 0.0026\n",
      "Epoch 12/20\n",
      "6/6 [==============================] - 2s 434ms/step - loss: 0.0074 - precision: 0.5036 - val_loss: 0.9552 - val_precision: 0.0026\n",
      "Epoch 13/20\n",
      "6/6 [==============================] - 2s 411ms/step - loss: 0.0064 - precision: 0.4910 - val_loss: 1.6676 - val_precision: 0.0026\n",
      "Epoch 14/20\n",
      "6/6 [==============================] - 2s 430ms/step - loss: 0.0058 - precision: 0.5036 - val_loss: 0.9366 - val_precision: 0.0026\n",
      "Epoch 15/20\n",
      "6/6 [==============================] - 2s 401ms/step - loss: 0.0092 - precision: 0.4910 - val_loss: 1.6096 - val_precision: 0.0026\n",
      "Epoch 16/20\n",
      "6/6 [==============================] - 2s 397ms/step - loss: 0.0060 - precision: 0.5000 - val_loss: 1.7421 - val_precision: 0.0026\n",
      "Epoch 17/20\n",
      "6/6 [==============================] - 2s 396ms/step - loss: 0.0045 - precision: 0.4964 - val_loss: 1.3226 - val_precision: 0.0026\n",
      "Epoch 18/20\n",
      "6/6 [==============================] - 2s 387ms/step - loss: 0.0054 - precision: 0.4964 - val_loss: 1.0804 - val_precision: 0.0026\n",
      "Epoch 19/20\n",
      "6/6 [==============================] - 2s 421ms/step - loss: 0.0075 - precision: 0.5018 - val_loss: 1.5273 - val_precision: 0.0026\n",
      "Epoch 20/20\n",
      "6/6 [==============================] - 2s 411ms/step - loss: 0.0058 - precision: 0.4982 - val_loss: 0.8151 - val_precision: 0.0026\n"
     ]
    }
   ],
   "source": [
    "adam = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=[precision])\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    epochs=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "26945b76-15bc-48a7-86d7-fd8d683e194f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8151329159736633, 0.0026070252060890198]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4156366a-f9e1-4a44-97cf-77f06cf70736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.82\n",
      "precision: 0.26%\n"
     ]
    }
   ],
   "source": [
    "print(\"%s: %.2f\" % (model.metrics_names[0], scores[0]))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cf45785f-4ae2-42c8-b8fb-50e08701f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9580e850-c0e7-4a5e-9a83-de0bb1a9d041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# If you don't have the following installed, from command line '!pip install scikeras'\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.initializers import GlorotNormal\n",
    "\n",
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "def build_clf(hidden_layer_sizes, dropout):\n",
    "    ann = tf.keras.models.Sequential()\n",
    "    ann.add(keras.layers.Input(shape=1472)),\n",
    "    for hidden_layer_size in hidden_layer_sizes:\n",
    "        model.add(keras.layers.Dense(hidden_layer_size, kernel_initializer= tf.keras.initializers.GlorotUniform(), \n",
    "                                     bias_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None), activation=\"relu\"))\n",
    "        model.add(keras.layers.Dropout(dropout))\n",
    "    ann.add(tf.keras.layers.Dense(10, activation='softmax',  kernel_initializer='glorot_normal' ))\n",
    "    ann.compile(loss = 'sparse_categorical_crossentropy', metrics = ['precision'])\n",
    "    return ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c8ddfb86-f0c1-4cbf-8189-03deea3d93c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "keras_clf = KerasClassifier(\n",
    "    model=build_clf,\n",
    "    hidden_layer_sizes=1472,\n",
    "    dropout = 0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "66254de8-d698-4e99-a558-25acecdd1fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'build_fn', 'warm_start', 'random_state', 'optimizer', 'loss', 'metrics', 'batch_size', 'validation_batch_size', 'verbose', 'callbacks', 'validation_split', 'shuffle', 'run_eagerly', 'epochs', 'hidden_layer_sizes', 'dropout', 'class_weight'])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "params = {\n",
    "    'optimizer__learning_rate': [0.0005, 0.001, 0.005],\n",
    "    'model__hidden_layer_sizes': [(70,),(90, ), (100,), (100, 90)],\n",
    "    'model__dropout': [0, 0.1],\n",
    "    'batch_size':[20, 60, 100],\n",
    "    'epochs':[10, 50, 100],\n",
    "    'optimizer':[\"adam\",'sgd']\n",
    "}\n",
    "keras_clf.get_params().keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6441efea-1806-4a20-8788-accdaae0c066",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "250 fits failed out of a total of 250.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "250 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\scikeras\\wrappers.py\", line 1494, in fit\n",
      "    super().fit(X=X, y=y, sample_weight=sample_weight, **kwargs)\n",
      "  File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\scikeras\\wrappers.py\", line 762, in fit\n",
      "    self._fit(\n",
      "  File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\scikeras\\wrappers.py\", line 931, in _fit\n",
      "    self._fit_keras_model(\n",
      "  File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\scikeras\\wrappers.py\", line 526, in _fit_keras_model\n",
      "    hist = self.model_.fit(x=X, y=y, **fit_args)\n",
      "  File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"C:\\Users\\Srinidhi\\AppData\\Local\\Temp\\__autograph_generated_file2em3ypcy.py\", line 15, in tf__train_function\n",
      "    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n",
      "ValueError: in user code:\n",
      "\n",
      "    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n",
      "        outputs = model.train_step(data)\n",
      "    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 998, in train_step\n",
      "        return self.compute_metrics(x, y, y_pred, sample_weight)\n",
      "    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1092, in compute_metrics\n",
      "        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n",
      "    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 577, in update_state\n",
      "        self.build(y_pred, y_true)\n",
      "    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 483, in build\n",
      "        self._metrics = tf.__internal__.nest.map_structure_up_to(\n",
      "    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 631, in _get_metric_objects\n",
      "        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n",
      "    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 631, in <listcomp>\n",
      "        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n",
      "    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 650, in _get_metric_object\n",
      "        metric_obj = metrics_mod.get(metric)\n",
      "    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\metrics\\__init__.py\", line 181, in get\n",
      "        return deserialize(str(identifier))\n",
      "    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\metrics\\__init__.py\", line 136, in deserialize\n",
      "        return deserialize_keras_object(\n",
      "    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\", line 769, in deserialize_keras_object\n",
      "        raise ValueError(\n",
      "\n",
      "    ValueError: Unknown metric function: precision. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\n",
      "\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 998, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1092, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 577, in update_state\n        self.build(y_pred, y_true)\n    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 483, in build\n        self._metrics = tf.__internal__.nest.map_structure_up_to(\n    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 631, in _get_metric_objects\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 631, in <listcomp>\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 650, in _get_metric_object\n        metric_obj = metrics_mod.get(metric)\n    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\metrics\\__init__.py\", line 181, in get\n        return deserialize(str(identifier))\n    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\metrics\\__init__.py\", line 136, in deserialize\n        return deserialize_keras_object(\n    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\", line 769, in deserialize_keras_object\n        raise ValueError(\n\n    ValueError: Unknown metric function: precision. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [72]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m earlystop \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m callback \u001b[38;5;241m=\u001b[39m [earlystop]\n\u001b[1;32m---> 13\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mrnd_search_cv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:926\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    924\u001b[0m refit_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 926\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scikeras\\wrappers.py:1494\u001b[0m, in \u001b[0;36mKerasClassifier.fit\u001b[1;34m(self, X, y, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m   1492\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight\n\u001b[0;32m   1493\u001b[0m     sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight, y\u001b[38;5;241m=\u001b[39my)\n\u001b[1;32m-> 1494\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scikeras\\wrappers.py:762\u001b[0m, in \u001b[0;36mBaseWrapper.fit\u001b[1;34m(self, X, y, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    757\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit__epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs)\n\u001b[0;32m    759\u001b[0m )\n\u001b[0;32m    760\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 762\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    763\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    764\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    765\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    766\u001b[0m     warm_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarm_start,\n\u001b[0;32m    767\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    768\u001b[0m )\n\u001b[0;32m    770\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scikeras\\wrappers.py:931\u001b[0m, in \u001b[0;36mBaseWrapper._fit\u001b[1;34m(self, X, y, sample_weight, warm_start, epochs, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    927\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_encoder_\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_model_compatibility(y)\n\u001b[1;32m--> 931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_keras_model(\n\u001b[0;32m    932\u001b[0m     X,\n\u001b[0;32m    933\u001b[0m     y,\n\u001b[0;32m    934\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    935\u001b[0m     warm_start\u001b[38;5;241m=\u001b[39mwarm_start,\n\u001b[0;32m    936\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[0;32m    937\u001b[0m     initial_epoch\u001b[38;5;241m=\u001b[39minitial_epoch,\n\u001b[0;32m    938\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    939\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scikeras\\wrappers.py:526\u001b[0m, in \u001b[0;36mBaseWrapper._fit_keras_model\u001b[1;34m(self, X, y, sample_weight, warm_start, epochs, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    524\u001b[0m         hist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_\u001b[38;5;241m.\u001b[39mfit(x\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_args)\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 526\u001b[0m     hist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_\u001b[38;5;241m.\u001b[39mfit(x\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_args)\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m warm_start \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistory_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m initial_epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory_ \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file2em3ypcy.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 998, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1092, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 577, in update_state\n        self.build(y_pred, y_true)\n    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 483, in build\n        self._metrics = tf.__internal__.nest.map_structure_up_to(\n    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 631, in _get_metric_objects\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 631, in <listcomp>\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 650, in _get_metric_object\n        metric_obj = metrics_mod.get(metric)\n    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\metrics\\__init__.py\", line 181, in get\n        return deserialize(str(identifier))\n    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\metrics\\__init__.py\", line 136, in deserialize\n        return deserialize_keras_object(\n    File \"C:\\Users\\Srinidhi\\anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\", line 769, in deserialize_keras_object\n        raise ValueError(\n\n    ValueError: Unknown metric function: precision. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(estimator=keras_clf, param_distributions=params, scoring='precision', n_iter=50, cv=5)\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(10000) # note: the default is 3000 (python 3.9)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')\n",
    "callback = [earlystop]\n",
    "\n",
    "_ = rnd_search_cv.fit(X_train, y_train, callbacks=callback, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2727cbb2-1419-4933-aa7a-12239e985f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdbvcvcrnd_search_cv.best_params_sdbvcvcrnd_search_cv.best_params_sdbvcvcrnd_search_cv.best_params_sdbvcvcccscssssssssaaaasaaaaaxxsacasxaasxxcv ccccxaccc vvvvvvvvvvvvvvvvvvvvvdbbbbbbccv cxzx cc cczx  z cv  xxxxxxxxxxxxxxxxxxcccCZCrnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a34ef6-265b-4c78-b9bd-9862f2ac0e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_net = rnd_search_cv.best_estimator_\n",
    "print(rnd_search_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d030d46d-f34c-4bb6-b5fc-06cbbf13ba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_pred = best_net.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2f1ddf-cc47-44c1-a671-91a719ff1810",
   "metadata": {},
   "source": [
    "## Inference:\n",
    "\n",
    "1. The f1 score for Random Search on Logistic Regression is 0.9762289562289563\n",
    "\n",
    "2. The f1 score for grid search on the logistic regression is 0.9742068308729206\n",
    "\n",
    "3. The f1 score for Random Search on Linear SVM is 0.9724053296218781\n",
    "\n",
    "4. The f1 score for Grid Search on Linear SVM is 0.9776157863464366\n",
    "\n",
    "5. The f1 score for Random Search on Poly SVM is 0.9736799574694313\n",
    "\n",
    "6. The f1 score for Grid Search on Poly SVM is 0.9724053296218781\n",
    "\n",
    "7. The f1 score for Random Search on RBF SVM is 0.9757976045282547\n",
    "\n",
    "8. The f1 score for Grid Search on RBF SVM is 0.9872873200854851\n",
    "\n",
    "9. The f1 score for Random Search on Decision tree classifier is 0.9891221824708063\n",
    "\n",
    "10. The f1 score for exhaustive Grid Search on Decision tree classifier is 0.9891221824708063\n",
    "\n",
    "11. The f1 score for Random Search on NN MLP Classifier is 0.9759743531909016\n",
    "\n",
    "12. The f1 score for Grid Search on on NN MLP Classifier is 0.9777758544419441\n",
    "\n",
    "When I try to include the Neural Networks using the MLP classifier into the model performing notebook, I tend to see that the NN is giving  the best f1 score of 97.59 for the Random Search training and 97.77 for the Grid Search. However when we compare the Grid Search scores of the MLP Classifier NN with that of the Grid search of the Decision Tree classifier or the RBF SVM classifier, it was comparitively low in the F1 score. This doesn't necessarily mean that the NN is not the better model but its not giving the best results compared to the decision tree or the SVM for this particular problem while fairly giving out the better end of the results. While the Neural Networks is a powerful model that works best for the complex problems,  I have tried my best to make the dataset as easier to analyse as possible by eliminating the complex relations in the dataset such as making the continuous variables which would have worked best with the MLP regressor and making them classified and also elimating the variables that could very well contribute to the complexity in the dataset; which might have worked for the NN to learn from this data; it could be possible that the Decision tree was able to predict the best results as it produces a tree like structure that can easily be analyzed. In conclusion, these results apply only for this dataset and doesn't necessarily be applied for the raw dataset where the more of the regression type of models are used. But based on this dataset results, I can certainly say that by including the MLP classifier, I did not see any change of the results from the previous iteration and the Decision Tree is still the best performing model compared to the rest of the models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
